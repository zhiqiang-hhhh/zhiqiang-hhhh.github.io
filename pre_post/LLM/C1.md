8-3

--------------
Hello everyone, so I've wanted to make this video for a while. It is a comprehensive but general audience introduction to large language modele like ChatGPT. And what i'm hoping to achieve in this video is to give you kind of mental models for thinking through what it is that this tool is. It is obviously magical and amazing in some respects. It is realy good at some things, not good at every things. And there is also lots of sharpe edges to be aware of.

So what is behind this text box? You can put anything in there, and press enter. But what should be putting there and what are those words generated back? How do this work and what are you talking to exactly? So I'm hoping to get all thoes topics in this video. We gonna go through the entire pipeline of how this stuff is built. But im going to keep every thing a sort of accessiable to a general audience.

So lets take a look at first how you built things like chatgpt and along the way i'm going to talk about some of the sort of cognitive psychological (认知心理学) implications of this tools.

There going to be mutiple stages arranged sequentially, the fist stage is called pre-train stage. And the fitst step of the pre-training stage is to download and process the internet. And to get a sence of what this roughly looks like, i recommand looking at the url here.

So, the company is called hugging face, collected and created this dataset called fine web. There going to be much details in the blogpost of how the they constructed this Fineweb dataset.
And all of the major LLM providers like openai and google will have some of the equivalent, internally, of something like Fineweb dataset.

So, roughly what we are trying to achieve here, we are trying to get ton of the text from the internet from public avaiable resources. so we are trying to have a huge quantity of very high quality ducuments. and we also want very large diversity of the ducuments. because we want to have a lot of knowledge inside of these models. So we want large diversity of high quality ducuments. And we want many many of them. And achieveing these is quiet completed and as you can see here, it takes mutiple stages to do well. 

--------------

8-4

--------------

So lets take a look at of what some of these stages looks like in a bit. For now i'd like to just like to note for example the fine web dataset which is fairly representive of what you would see in a production grade application, ending up with 44 tb of disk spaces. you can get a usb stick for like tera-bytes very easily or i think this could fit into a single hard-drive almost today. So this is not a huge amount of data at the end of the day. Even though the internet is very very large, we are working with text and we also filter it aggressively so we end up with but? 44 tera-bytes in this example.

So lets take a look at what this data looks like, and what part of the stages are so hard. The start point of this lots of efforts and something that contributes most to the data by the end of it, is the data from common crawl. So common crawl is an orginazation that has been basicly scaring? the internet since 2007. As of 2024 for example, CC has indexed 2.7 billion web pages. They have all this crawlers going aroud the internet. And what it does basicly is you start with a few seed few web pages and you follow all the links. and you just keep following the pages and you keep indexing all the information and you end up with ton of the data in the internet. So this is usually the start point of all these efforts.

And now these common crawl data is quiet raw and filted with many many different ways. They document a little bit the kind of processing that happens in this stages. So the first this here is something called URL filtering. so what this is reffering to is that there is a block list of url or domoins that you usually dont want to be geting data from. So this usually includes data like malware websites, spainish websites, marketing websites and racis websites and adults websites and thing like that. so there is some different types of websites that they are just elimated at this stage. because we dont want them in our data set.

The second parts is text extraction. You have to remember that all this webpages is raw webpages that have been saved by these crawlers. So i go to inspect here, this is the raw html actually looks like.
We really want is text, we dont want nivigation or things like that. So there is a lot of filtering and processing.
The next stage here is language filtering. So for example, fine web filters using the language clasfier they try to guess what language every single webpages is in, and they only keep web pages that has more thant 65% english for example. this is a design decision that different companies can take for themselves.

--------------