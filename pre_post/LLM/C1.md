8-3

--------------
Hello everyone, so I've wanted to make this video for a while. It is a comprehensive but general audience introduction to large language modele like ChatGPT. And what i'm hoping to achieve in this video is to give you kind of mental models for thinking through what it is that this tool is. It is obviously magical and amazing in some respects. It is realy good at some things, not good at every things. And there is also lots of sharpe edges to be aware of.

So what is behind this text box? You can put anything in there, and press enter. But what should be putting there and what are those words generated back? How do this work and what are you talking to exactly? So I'm hoping to get all thoes topics in this video. We gonna go through the entire pipeline of how this stuff is built. But im going to keep every thing a sort of accessiable to a general audience.

So lets take a look at first how you built things like chatgpt and along the way i'm going to talk about some of the sort of cognitive psychological (认知心理学) implications of this tools.

There going to be mutiple stages arranged sequentially, the fist stage is called pre-train stage. And the fitst step of the pre-training stage is to download and process the internet. And to get a sence of what this roughly looks like, i recommand looking at the url here.

So, the company is called hugging face, collected and created this dataset called fine web. There going to be much details in the blogpost of how the they constructed this Fineweb dataset.
And all of the major LLM providers like openai and google will have some of the equivalent, internally, of something like Fineweb dataset.

So, roughly what we are trying to achieve here, we are trying to get ton of the text from the internet from public avaiable resources. so we are trying to have a huge quantity of very high quality ducuments. and we also want very large diversity of the ducuments. because we want to have a lot of knowledge inside of these models. So we want large diversity of high quality ducuments. And we want many many of them. And achieveing these is quiet completed and as you can see here, it takes mutiple stages to do well. 

--------------

8-4

--------------

So lets take a look at of what some of these stages looks like in a bit. For now i'd like to just like to note for example the fine web dataset which is fairly representive of what you would see in a production grade application, ending up with 44 tb of disk spaces. you can get a usb stick for like tera-bytes very easily or i think this could fit into a single hard-drive almost today. So this is not a huge amount of data at the end of the day. Even though the internet is very very large, we are working with text and we also filter it aggressively so we end up with but? 44 tera-bytes in this example.

So lets take a look at what this data looks like, and what part of the stages are so hard. The start point of this lots of efforts and something that contributes most to the data by the end of it, is the data from common crawl. So common crawl is an orginazation that has been basicly scaring? the internet since 2007. As of 2024 for example, CC has indexed 2.7 billion web pages. They have all this crawlers going aroud the internet. And what it does basicly is you start with a few seed few web pages and you follow all the links. and you just keep following the pages and you keep indexing all the information and you end up with ton of the data in the internet. So this is usually the start point of all these efforts.

And now these common crawl data is quiet raw and filted with many many different ways. They document a little bit the kind of processing that happens in this stages. So the first this here is something called URL filtering. so what this is reffering to is that there is a block list of url or domoins that you usually dont want to be geting data from. So this usually includes data like malware websites, spainish websites, marketing websites and racis websites and adults websites and thing like that. so there is some different types of websites that they are just elimated at this stage. because we dont want them in our data set.

The second parts is text extraction. You have to remember that all this webpages is raw htmls that have been saved by these crawlers. So i go to inspect here, this is what the raw html actually looks like. You'll notice that it gets all this marks up. like list and stuff like that and CSS and all thess kind of stuffs. So this is the compute codes of these web pages. But what we really want is that, we just want this text, we just want the text of this web pages, we dont want the nivigation and the things like that. So there is a lot of filtering and processing and going to adauxxxly filtering for just good conntent of these web pages.

The next stage here is language filtering. So for example, fineweb, filters - using the language clasfier they try to guess what language every single webpages is in, and they only keep web pages that has more thant 65% english as an example. this is a design decision that different companies can take for themselves.

--------------

8-5

--------------

What fractions of all these different languages are we going to include in our dataset. Because for example we are going to filtering out all these spainish for example. Then you might imagine that model latei will not be very good at spainish becaseu it's just never seen that much of data of that language. Do different companies can focus on mutilanguage performance to a different degree as an example. So fine web is quite focused on english and so their language model if they end up training later will be very good at english but not will be very good at other languages.

After language filtering there is a few other filtering steps and D duplication and things like that finishing for example that pii removal this is personally identifiable information so as an example addresses social security numbers and things like that you would try to  detect them and you would try to filter out this kind of web pages from dataset as well. so there is a lot of stages here and i won't go into detail but it is a fairly extensive part of the pre-procesing. And you end up for example with the fine web datase. And so when you click on it and you can see some examples here of what this actually looking like. and anyone can download it on the hugging face web pages. And so there is some examples of final text that ends up at the training site.

So this is some artical about tornados and 2012, so there are some tornados in 2012 and what happened, the second is something about did you two little yellow xxx.. So just think of these as basically web pages in the internet filted so for the text in various ways. and now we have a tons of texts. Fourty terabytes offit. An now is the staring point of the next stage. And now i want to give you an intuitive sense of where we are right now, so I first take the first two hundrend web pages, and remember we have tons of them. and i just take all of them and put it all together, concatnated. And so this is what we end up with. we just got raw text, raw internet text. And there is ton of it Even if in the 200 web pages. So i can continue zomming out here. and we just have this massive tapestry(?) of these texy data. And these text data has this patterns. and what we want to do now is we want to start tring your own neural networks on these data, so the neurals can internalise and model how these text flows. and we just have these gaint textual of text. and now we want to get neural nets starts to mimic it.